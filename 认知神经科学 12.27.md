# Methods
## Dimension Reduction
### Unsupervised Tranditional Methods
#### Kernel PCA Method

Kernel Principal Component Analysis (Kernel PCA) is a non-linear dimensionality reduction technique that extends the traditional Principal Component Analysis to handle non-linear relationships in data. It achieves this by utilizing kernel functions to map the input data into a higher-dimensional feature space, where linear relationships are more apparent. This allows for capturing complex patterns and structures that may be overlooked by linear PCA.

##### Basic Idea

The basic idea behind Kernel PCA is to find the principal components in the transformed feature space rather than the original input space. This is achieved through the following steps:

1. **Kernel Function Application:** Apply a chosen kernel function (e.g., radial basis function, polynomial) to map the input data into a higher-dimensional space.

2. **Centering:** Center the data in the feature space to ensure that the principal components are based on the covariance matrix of the transformed data.

3. **Eigendecomposition:** Perform eigendecomposition on the covariance matrix of the transformed data to obtain the principal components.

4. **Projection:** Project the original data onto the principal components in the higher-dimensional space to obtain the final reduced-dimensional representation.

##### Mathematical Formulation

The kernel trick is employed to compute the dot products in the high-dimensional space efficiently without explicitly calculating the transformed data points. The kernel PCA algorithm involves solving the eigenvector problem for the kernel matrix.

The kernel PCA can be formulated as follows:

$$K_{ij} = \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j) $$

where $K$ is the kernel matrix, $\phi(\mathbf{x}_i)$ and $\phi(\mathbf{x}_j)$ are the mapped feature vectors in the high-dimensional space for data points $\mathbf{x}_i$ and $\mathbf{x}_j$, respectively.

##### Parameters and Hyperparameters

Kernel PCA involves hyperparameters such as the choice of the kernel function and its associated parameters (e.g., kernel width for the radial basis function). The selection of these hyperparameters can significantly impact the performance of Kernel PCA, and grid search or cross-validation is often employed to find optimal values.

In conclusion, Kernel PCA is a powerful tool for non-linear dimensionality reduction, particularly when dealing with complex data structures and relationships.

#### Isomap Method
Isomap (Isometric Mapping) is a non-linear dimensionality reduction technique that aims to preserve the intrinsic geometry of high-dimensional data in a lower-dimensional space. It addresses the limitations of linear methods, such as PCA, by capturing the underlying manifold structure in the data.

##### Basic Idea

The fundamental concept behind Isomap is to represent the data points on a manifold as nodes in a graph, where the edges between nodes correspond to pairwise geodesic distances. Geodesic distances, which represent the shortest path along the manifold, are used to construct a neighborhood graph. Isomap then computes a low-dimensional embedding by preserving these pairwise geodesic distances as accurately as possible in the reduced space.

##### Algorithm Steps

1. **Neighborhood Graph Construction:**
   - Calculate pairwise Euclidean distances between data points.
   - For each data point, identify its k-nearest neighbors based on Euclidean distances.
   - Construct a neighborhood graph, where nodes represent data points, and edges connect points that are k-nearest neighbors.

2. **Geodesic Distance Approximation:**
   - Estimate geodesic distances between nodes in the graph using graph algorithms such as Dijkstra's algorithm.

3. **Isomap Embedding:**
   - Perform classical multidimensional scaling (MDS) on the matrix of approximate geodesic distances to obtain the low-dimensional representation.

##### Mathematical Formulation

The Isomap algorithm involves several mathematical components, including the computation of pairwise Euclidean distances, geodesic distances, and the application of classical MDS. The key equations include:

- Euclidean Distance: $d_{ij} = \lVert \mathbf{x}_i - \mathbf{x}_j \rVert_2$
- Geodesic Distance Approximation: Use graph algorithms to estimate geodesic distances.
- MDS Objective Function: Minimize the discrepancy between pairwise geodesic distances and distances in the low-dimensional embedding.

##### Parameters and Considerations

- **Neighborhood Size (k):** The choice of the parameter k influences the local connectivity of the data points. It is often selected through cross-validation or based on domain knowledge.
  
- **Dimensionality of Embedding Space:** Researchers need to decide on the desired dimensionality of the reduced space based on the characteristics of the data and the goals of the analysis.




### Supervised Deep Learning Methods
#### Resnet18 Backbone
We consider the input (b,N) feature as a image and reshape it into (b,3,H,W) format. Then the image is fed into a classical CNN architecture: resnet18 backbone.

ResNet-18, short for Residual Network with 18 layers, is a deep neural network architecture that has proven to be highly effective in various computer vision tasks, particularly in image classification. Introduced by He et al. in the paper "Deep Residual Learning for Image Recognition" (2016), ResNet-18 is part of the ResNet family known for its innovative use of residual connections to address the vanishing gradient problem in deep networks.

##### Architecture

ResNet-18 consists of a stack of residual blocks, each containing a few convolutional layers. The key innovation lies in the inclusion of skip connections or shortcuts that bypass one or more layers. These skip connections enable the gradient to flow directly through the network, facilitating the training of very deep networks.

##### Basic Residual Block

A basic residual block in ResNet-18 has the following structure:
```plaintext
Input --> Convolution --> Batch Normalization --> ReLU --> Convolution --> Batch Normalization --> Skip Connection --> ReLU
```
##### Advantages

ResNet-18 offers several advantages as a backbone architecture:

- **Ease of Training:** The use of residual connections mitigates the vanishing gradient problem, allowing for the training of very deep networks.

- **Improved Performance:** Deeper architectures enable the learning of more complex features, leading to improved performance on various tasks.

- **Transferability:** Pre-trained ResNet-18 models on large datasets (e.g., ImageNet) can be used as powerful feature extractors for other tasks via transfer learning.

##### Implementation Details

When using ResNet-18 as a backbone in a specific task, one can leverage pre-trained models or fine-tune them on task-specific data. The architecture can be adjusted based on the requirements of the target task, such as the number of output classes.
```python
import torch
import torchvision.models as models

# Load pre-trained ResNet-18 model
resnet18 = models.resnet18(pretrained=True)
````
#### IVIS Dimensionality Reduction

IVIS is a machine learning library for reducing dimensionality of very large datasets using Siamese Neural Networks. IVIS preserves global data structures in a low-dimensional space, adds new data points to existing embeddings using a parametric mapping function, and scales linearly to millions of observations. The algorithm is described in detail in [Structure-preserving visualisation of high dimensional single-cell datasets](https://www.nature.com/articles/s41598-019-45301-0).

IVIS utilizes a siamese neural network(SNNS) architecture that is trained using a novel triplet loss function. SNNs are a class of neural network that employ a unique architecture to naturally rank similarity between inputs. The IVIS SNN consists of three identical base networks; each base network has three dense layers of 128 neurons followed by a final embedding layer. The size of the embedding layer reflects the desired dimensionality of outputs; results presented in this work utilize a final embedding layer with two neurons.

The layers preceding the embedding layer use the SELU activation function,
$$selu(x)=\lambda \{\begin{array}{lc}x & if\,x > 0\\ \alpha {e}^{x}-\alpha  & if\,x\le 0\end{array}$$
The loss function used to train the network is a variant of the standard triplet loss function:
$${L}_{tri}(\theta )={[\sum _{a,p,n}{D}_{a,p}-min({D}_{a,n},{D}_{p,n})+m]}_{+}$$
where _a_, _p_, and _n_ correspond to anchor, positive, and negative points respectively, _D_ is the Euclidean distance, and _m_ is the margin. The Euclidean distance D reflects similarity between points _a_ and _b_ in the embedding space.
$${D}_{a,b}=\sqrt{\sum _{i=1}^{n}{({a}_{i}-{b}_{i})}^{2}}$$
The SNN was trained on mini-batches of size 128 for 1000 epochs using the Adam optimizer function with a learning rate of 0.001 and standard parameters ($\beta_1$=0.9,$\beta_2$=0.999). Training was halted early if the loss failed to decrease over 50 consecutive epochs.
## Multimodal Fusion

Three types of features, rsfc_atlas, rsfc_yeo, scfp_atlas, are provided. We consider them as three modals and uitilize multimodal fusion methods to fuse these features.

We first use a simple MLP block to reencode the features.
### Encoder

```python
class MLPBlock(nn.Module):
	def __init__(self, in_features, out_features, dropout=0.2):
		super(MLPBlock, self).__init__()
		self.linear = nn.Linear(in_features, out_features)
		self.dropout = nn.Dropout(dropout)
		self.relu = nn.ReLU()

	def forward(self, x):
		x = self.linear(x)
		x = self.dropout(x)
		x = self.relu(x)
		return x

class Encoder(nn.Module):
	def __init__(self, in_features, out_features, dropout=0.2):
		super(Encoder, self).__init__()
		self.mlp = MLPBlock(in_features, out_features, dropout=dropout)
		self.linear = nn.Linear(out_features, out_features)
		self.dropout = nn.Dropout(dropout)
		self.relu = nn.ReLU()

	def forward(self, x):
		hidden_state = self.mlp(x)
		x = self.linear(hidden_state)
		x = self.dropout(x)
		x = self.relu(x)
		return x, hidden_state
```

The output features and hidden_states are further fused using three different methods.

### Naive Concat

The features from three modalities are simple concated to form the final features.

```python
class NaiveCatFusion(nn.Module):
    def __init__(self, in_modal_list, out_features, dropout=0.2):
        super(NaiveCatFusion, self).__init__()
        self.encoder_list = nn.ModuleList()
        for in_features in in_modal_list:
            self.encoder_list.append(Encoder(in_features, out_features//len(in_modal_list), dropout=dropout))
        self.out_features = out_features

    def forward(self, x_list):
        assert len(x_list) == len(self.encoder_list)
        out_feature_list = []
        for x, encoder in zip(x_list, self.encoder_list):
            out_feature, _ = encoder(x)
            out_feature_list.append(out_feature)
        out_feature = torch.cat(out_feature_list, dim=1)
        return out_feature
```
### Attension Fusion

We use multi-head attention block to fuse features. Hidden states of each modal work as a query and search for relationship from other two modals. THe outputs, with original features, are then concated to form the final feature.

```python
class AttentionFusion(nn.Module):
    def __init__(self, in_modal_list, out_features, dropout=0.2, num_heads=4):
        super(AttentionFusion, self).__init__()
        self.encoder_list = nn.ModuleList()
        for in_features in in_modal_list:
            self.encoder_list.append(Encoder(in_features, out_features//len(in_modal_list), dropout=dropout))
        self.out_features = out_features

        # multi-head attention
        self.num_heads = num_heads
        self.attention_list =nn.ModuleList()
        for i in range(len(in_modal_list)):
            self.attention_list.append(nn.MultiheadAttention(out_features//len(in_modal_list), num_heads=num_heads))

    def forward(self, x_list):
        assert len(x_list) == len(self.encoder_list)
        out_feature_list = []
        hidden_state_list = []
        for x, encoder in zip(x_list, self.encoder_list):
            out_feature, hidden_state = encoder(x)
            out_feature_list.append(out_feature)
            hidden_state_list.append(hidden_state)
        
        # multi-head attention
        attention_out_list = []
        for i in range(len(x_list)):
            # every modal attends to all other modals
            attention_list=[]
            for j in range(len(x_list)):
                if i != j:
                    attention_out, _ = self.attention_list[i](hidden_state_list[i], hidden_state_list[j], hidden_state_list[j])
                    attention_list.append(attention_out)
            attention_out = torch.cat(attention_list, dim=1)
            attention_out_list.append(attention_out)
        
        # concat
        out_list=[]
        for i in range(len(x_list)):
            out = torch.cat([out_feature_list[i], attention_out_list[i]], dim=1)
            out_list.append(out)

        out_feature = torch.cat(out_list, dim=1)

        return out_feature
```

### Transformer Encoder

It's similar with **Attension Fusion**, but instead of feeding hidden states into attention block, in this section we feed features into a whole transformer.

```python
class TransformerFusion(nn.Module):
    def __init__(self, in_modal_list, out_features, nhead, dropout=0.2):
        super(TransformerFusion, self).__init__()
        self.encoder_list = nn.ModuleList()
        for in_features in in_modal_list:
            self.encoder_list.append(Encoder(in_features, out_features//len(in_modal_list), dropout=dropout))
        self.out_features = out_features

        self.attention = nn.TransformerEncoderLayer(
            d_model=out_features,
            nhead=nhead,
            dropout=dropout
        )

    def forward(self, x_list):
        assert len(x_list) == len(self.encoder_list)
        out_feature_list = []
        for x, encoder in zip(x_list, self.encoder_list):
            out_feature, hidden_state = encoder(x)
            out_feature_list.append(out_feature)
        attention_out= self.attention(torch.cat(
            out_feature_list,
        dim=1).unsqueeze(1))

        return attention_out.squeeze(1)
```


# Experiments

Task: PicSeq_Unadj
## Baseline

### PCA+xgboost
```plain_text
MAE Loss:  11.545683607356565
Corr:  -0.07005312350729423
```
### Isomap+xgboost

```plain_text
MAE Loss:  11.820261125503832
Corr:  0.007823219732170678
```
### ivis+xgboost
```plain_text
MAE Loss:  11.913941825332158
Corr:  0.0725339461294053
```
## Ours
### PCA + Naive Concat
```plain_text
MAE Loss:  11.107340882902692
Corr:  0.10934553491370473
```
### PCA + Attention Fusion
```plain_text
MAE Loss:  11.108754179158788
Corr:  0.06423691728018094
```
### PCA + Transformer Encoder
```plain_text
MAE Loss:  11.110194336715018
Corr:  -0.07876285667046642
```
### Isomap + Naive Concat
```plain_text
MAE Loss:  13.177885561414586
Corr:  0.009611993530814121
```
### Isomap + Attention Fusion
```plain_text
MAE Loss:  11.244915297198448
Corr:  -0.1177094089505038
```
### Isomap + Transformer Encoder
```plain_text
MAE Loss:  11.260123819727808
Corr:  0.04364916228887163
```
### ivis + Naive Concat
```plain_text
MAE Loss:  11.063255867927698
Corr:  0.12360766724408058
```
### ivis + Attention Fusion
```plain_text
MAE Loss:  11.141801454580513
Corr:  0.0634460389560033
```
### ivis + Transformer Encoder
```plain_text
MAE Loss:  11.110229945091685
Corr:  0.10117607007733255
```
### resnet + Naive Concat
```plain_text
MAE Loss:  11.044170333959494
Corr:  0.07506315743690578
```
### resnet + Attention Fusion
```plain_text
MAE Loss:  11.00211259198037
Corr:  0.09263260303423734
```
### resnet + Transformer Encoder
```plain_text
MAE Loss:  11.112543012291002
Corr:  0.10140894391877678
```

### GCN + Naive Concat
```plain_text
MAE Loss:  11.742089830143438
Corr:  -0.06263390809871335
```
### GCN + Attention Fusion
```plain_text
MAE Loss:  12.343601822579744
Corr:  0.047178052660140544
```
### GCN + Transformer Encoder
```plain_text
MAE Loss:  11.118998306177224
Corr:  0.013128591861695828
```

